{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"./final/\" # folder containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 class\n",
      "Loaded 0 class\n",
      "Loaded 7 class\n",
      "Loaded 6 class\n",
      "Loaded 1 class\n",
      "Loaded 8 class\n",
      "Loaded 4 class\n",
      "Loaded 3 class\n",
      "Loaded 2 class\n",
      "Loaded 5 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.00784314 0.\n",
      " 0.03921569 0.5137255  0.39607844 0.44313726 0.49411765 0.4\n",
      " 0.23529412 0.2509804  0.33333334 0.3372549  0.36862746 0.35686275\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.21176471 0.53333336\n",
      " 0.6313726  0.7607843  0.7882353  0.85882354 0.84705883 0.8352941\n",
      " 0.85490197 0.78431374 0.77254903 0.68235296 0.02352941 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.5372549  0.52156866 0.20392157 0.29411766 0.5372549\n",
      " 0.58431375 0.49019608 0.59607846 0.5137255  0.3372549  0.7137255\n",
      " 0.6901961  0.7019608  0.13725491 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.         0.10588235 0.80784315\n",
      " 0.7137255  0.7019608  0.56078434 0.5686275  0.60784316 0.40392157\n",
      " 0.44313726 0.5254902  0.48235294 0.40392157 0.36078432 0.56078434\n",
      " 0.34901962 0.         0.         0.         0.         0.\n",
      " 0.00392157 0.         0.         0.         0.00392157 0.01568628\n",
      " 0.         0.00784314 0.49411765 0.19607843 0.4745098  0.77254903\n",
      " 0.7176471  0.64705884 0.70980394 0.827451   0.84705883 0.65882355\n",
      " 0.52156866 0.7490196  0.68235296 0.6627451  0.5372549  0.\n",
      " 0.         0.         0.         0.00392157 0.00392157 0.\n",
      " 0.         0.         0.00784314 0.01960784 0.         0.30980393\n",
      " 0.6901961  0.49411765 0.49019608 0.53333336 0.7372549  0.70980394\n",
      " 0.7254902  0.7019608  0.87058824 0.5137255  0.6784314  1.\n",
      " 0.84313726 0.7137255  0.8392157  0.20392157 0.         0.\n",
      " 0.01176471 0.01568628 0.01176471 0.01568628 0.01568628 0.00784314\n",
      " 0.         0.         0.18039216 0.48235294 0.40392157 0.48235294\n",
      " 0.54509807 0.6        0.61960787 0.7882353  0.8156863  0.73333335\n",
      " 0.6745098  0.4117647  0.5137255  0.72156864 0.7254902  0.7254902\n",
      " 0.8745098  0.18431373 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19607843\n",
      " 0.46666667 0.39607844 0.35686275 0.3764706  0.41960785 0.5372549\n",
      " 0.5568628  0.6        0.7294118  0.8039216  0.84313726 0.70980394\n",
      " 0.7019608  0.7647059  0.7647059  0.7019608  0.7647059  0.00392157\n",
      " 0.         0.         0.08235294 0.13333334 0.15294118 0.10196079\n",
      " 0.05882353 0.16470589 0.33333334 0.42745098 0.3647059  0.4745098\n",
      " 0.4        0.43529412 0.47058824 0.4745098  0.50980395 0.5294118\n",
      " 0.5411765  0.627451   0.6901961  0.79607844 0.6745098  0.54901963\n",
      " 0.6627451  0.59607846 0.7921569  0.24313726 0.         0.44313726\n",
      " 0.45882353 0.5137255  0.5137255  0.47058824 0.43137255 0.4627451\n",
      " 0.4509804  0.41960785 0.43137255 0.4745098  0.56078434 0.5137255\n",
      " 0.5647059  0.52156866 0.5568628  0.5137255  0.49019608 0.5294118\n",
      " 0.6392157  0.6784314  0.59607846 0.6156863  0.54509807 0.5294118\n",
      " 0.6509804  0.08627451 0.31764707 0.6509804  0.5137255  0.4862745\n",
      " 0.4509804  0.39607844 0.40784314 0.45490196 0.4745098  0.4627451\n",
      " 0.5137255  0.49019608 0.5254902  0.5882353  0.5019608  0.48235294\n",
      " 0.5764706  0.6        0.6666667  0.65882355 0.75686276 0.67058825\n",
      " 0.68235296 0.6627451  0.6        0.6        0.6156863  0.01176471\n",
      " 0.07058824 0.45882353 0.63529414 0.6627451  0.6313726  0.5647059\n",
      " 0.49411765 0.45882353 0.4509804  0.4509804  0.4745098  0.5137255\n",
      " 0.56078434 0.5921569  0.6627451  0.6509804  0.6313726  0.7647059\n",
      " 0.7607843  0.65882355 0.63529414 0.5411765  0.5254902  0.5019608\n",
      " 0.52156866 0.5137255  0.6117647  0.05098039 0.         0.\n",
      " 0.14117648 0.4117647  0.58431375 0.6745098  0.7176471  0.67058825\n",
      " 0.6313726  0.58431375 0.627451   0.6745098  0.7411765  0.6745098\n",
      " 0.6392157  0.53333336 0.39607844 0.32941177 0.05882353 0.39215687\n",
      " 0.7921569  0.6392157  0.60784316 0.627451   0.6745098  0.69803923\n",
      " 0.81960785 0.16470589 0.         0.         0.         0.\n",
      " 0.         0.06666667 0.16862746 0.26666668 0.3647059  0.36862746\n",
      " 0.28235295 0.20392157 0.12156863 0.07450981 0.         0.\n",
      " 0.         0.         0.         0.22745098 0.58431375 0.38039216\n",
      " 0.36078432 0.3372549  0.28627452 0.25490198 0.3019608  0.01568628\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Now we try to load the images and their corresponding labels into memory\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        # Skip hidden files and ensure it's a directory\n",
    "        if f.startswith('.') or not os.path.isdir(os.path.join(folder, f)):\n",
    "            continue\n",
    "            \n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            # Skip hidden files in subdirectories too\n",
    "            if file.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # One-hot encoding prevents the model from assuming ordinal relationships between classes (e.g., class 5 is \"greater\" than class 3)\n",
    "            # It's required for cross-entropy loss calculation, which expects probability distributions\n",
    "            # The output layer produces probabilities for each class, so the true labels need to match this format\n",
    "            # One-hot vectors have a single 1 at the correct class position and 0s elsewhere, making the loss calculation straightforward\n",
    "            \n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "# Efficient vectorized operations for mathematical computations.\n",
    "# Memory efficiency compared to Python lists  Broadcasting capabilities for element-wise operations.\n",
    "# Matrix multiplication support essential for neural network computations.\n",
    "# Consistent data types across all elements\n",
    "\n",
    "# Convert x and y to numpy arrays here\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "# This slicing operation:  \n",
    "# Removes the color channel dimension - converts RGBA or RGB images to grayscale  \n",
    "# The original shape is (num_images, height, width, channels)  \n",
    "# After slicing: (num_images, height, width) - takes only the first channel  \n",
    "# This reduces data dimensionality while preserving the essential image information for classification\n",
    "\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data? \n",
    "# Reshaping from (num_images, 28, 28) to (num_images, 784):  \n",
    "# Flattens 2D images into 1D vectors required for fully connected neural network input  \n",
    "# Neural networks expect input as feature vectors, not 2D matrices  28 × 28 = 784 pixels become individual input features  \n",
    "# Enables matrix multiplication with the weight matrix\n",
    "\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?  # Example: learning_rate=0.01, balancing convergence speed and training stability.\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "        [Q6] Why are the dimensions of the weights and biases the way they are?  # Weights dims: wih(hidden,input), who(output,hidden); biases match neuron counts to enable correct matrix multiplications.\n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?  # Broadcasting expands bias shape (neurons,1) to match batch dimensions automatically for addition.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?  # np.random.randn draws from N(0,1); wih shape=(hidden_neurons,input_neurons), scaled by sqrt(2/input_neurons) for He init.\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?  # Activation funcs like ReLU add non-linearity, enabling learning complex patterns beyond linear transformations.\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it  # Softmax converts logits to class probabilities that sum to 1, necessary for multi-class cross-entropy loss.\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        z_stable = z - np.max(z, axis=0, keepdims=True)\n",
    "        return np.exp(z_stable) / np.sum(np.exp(z_stable), axis=0, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?  # Loss functions measure prediction error and guide weight updates; cross-entropy is preferred for classification tasks.\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross entropy loss: -sum(y * log(y_hat))\n",
    "        return -np.sum(y * np.log(y_hat_clipped), axis=0)\n",
    "\n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        # For cross entropy with softmax, the derivative simplifies to:\n",
    "        # dE/dz = y_hat - y (where z is the input to softmax)\n",
    "        return y_hat - y\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?  # Output of size 10 per image gives probability for each of 10 classes; highest indicates predicted class.\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?  # Centering inputs by subtracting mean improves convergence and prevents activation saturation.\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?  # Applying softmax ensures the outputs form a valid probability distribution over classes.\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "\n",
    "        # Return it\n",
    "        return final_outputs\n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?  # Recomputing forward pass ensures current activations and inputs for computing accurate backprop gradients.\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.cross_entropy_loss(tj, yj) # Using cross entropy loss\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "        # Using cross entropy loss derivative\n",
    "        dE_dzo = self.cross_entropy_derivative(tj, yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list, validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?  # Validation data checks model performance on unseen data, indicating generalization beyond training set.\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            # Using cross entropy loss for validation as well\n",
    "            val_predictions = self.forward(validation_data)\n",
    "            vloss = self.cross_entropy_loss(validation_labels.T, val_predictions)\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss, val_loss \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.4453228425306417, Val Loss: 2.4159322434812016\n",
      "Epoch: 1, Loss: 2.41676883642841, Val Loss: 2.3881367667208955\n",
      "Epoch: 2, Loss: 2.388975944829979, Val Loss: 2.361079479614758\n",
      "Epoch: 3, Loss: 2.361919840354067, Val Loss: 2.334727160514906\n",
      "Epoch: 4, Loss: 2.3355838273225125, Val Loss: 2.30907079940331\n",
      "Epoch: 5, Loss: 2.3099452172665615, Val Loss: 2.2840899507617376\n",
      "Epoch: 6, Loss: 2.2849810561222723, Val Loss: 2.2597568773243535\n",
      "Epoch: 7, Loss: 2.260677101332634, Val Loss: 2.236060544400747\n",
      "Epoch: 8, Loss: 2.2370072030830426, Val Loss: 2.212977001051978\n",
      "Epoch: 9, Loss: 2.2139496864305617, Val Loss: 2.19049274553571\n",
      "Epoch: 10, Loss: 2.1914955545751846, Val Loss: 2.1686109733977497\n",
      "Epoch: 11, Loss: 2.1696486119814185, Val Loss: 2.1473089536114562\n",
      "Epoch: 12, Loss: 2.148381576708831, Val Loss: 2.126575584649816\n",
      "Epoch: 13, Loss: 2.127700519480205, Val Loss: 2.1064235228871966\n",
      "Epoch: 14, Loss: 2.107592949340894, Val Loss: 2.0868191280958204\n",
      "Epoch: 15, Loss: 2.088025040189831, Val Loss: 2.067750918650029\n",
      "Epoch: 16, Loss: 2.0689816504073315, Val Loss: 2.0491721433867336\n",
      "Epoch: 17, Loss: 2.0504279278849364, Val Loss: 2.031075679875727\n",
      "Epoch: 18, Loss: 2.032349689268848, Val Loss: 2.013443169585531\n",
      "Epoch: 19, Loss: 2.0147360782916217, Val Loss: 1.996246889113535\n",
      "Epoch: 20, Loss: 1.9975508931189578, Val Loss: 1.9794654377862893\n",
      "Epoch: 21, Loss: 1.9807674545436007, Val Loss: 1.963076467532211\n",
      "Epoch: 22, Loss: 1.964363307955221, Val Loss: 1.9470541985995224\n",
      "Epoch: 23, Loss: 1.948322495100978, Val Loss: 1.9313881732719838\n",
      "Epoch: 24, Loss: 1.9326397357864535, Val Loss: 1.9160582514101319\n",
      "Epoch: 25, Loss: 1.9172971211986276, Val Loss: 1.9010494792638937\n",
      "Epoch: 26, Loss: 1.9022769217598698, Val Loss: 1.8863489122043344\n",
      "Epoch: 27, Loss: 1.8875627360192822, Val Loss: 1.8719490948909776\n",
      "Epoch: 28, Loss: 1.8731455880582824, Val Loss: 1.8578430383802684\n",
      "Epoch: 29, Loss: 1.859017936481973, Val Loss: 1.8440125707315917\n",
      "Epoch: 30, Loss: 1.845163046136357, Val Loss: 1.830446015535973\n",
      "Epoch: 31, Loss: 1.8315744665638627, Val Loss: 1.817140198287006\n",
      "Epoch: 32, Loss: 1.8182399331505028, Val Loss: 1.8040825118390775\n",
      "Epoch: 33, Loss: 1.8051558531007696, Val Loss: 1.7912653311867057\n",
      "Epoch: 34, Loss: 1.7923107191734418, Val Loss: 1.7786796498899808\n",
      "Epoch: 35, Loss: 1.7796962045297355, Val Loss: 1.7663214925077857\n",
      "Epoch: 36, Loss: 1.767308605152447, Val Loss: 1.754176688296262\n",
      "Epoch: 37, Loss: 1.755136352761246, Val Loss: 1.742242016131265\n",
      "Epoch: 38, Loss: 1.7431750592762698, Val Loss: 1.730514558170776\n",
      "Epoch: 39, Loss: 1.7314234019082042, Val Loss: 1.718983769434257\n",
      "Epoch: 40, Loss: 1.7198693703937973, Val Loss: 1.7076496741385412\n",
      "Epoch: 41, Loss: 1.7085090097820943, Val Loss: 1.6965033299106957\n",
      "Epoch: 42, Loss: 1.697335919856766, Val Loss: 1.685540836410029\n",
      "Epoch: 43, Loss: 1.686345942771742, Val Loss: 1.6747526236123376\n",
      "Epoch: 44, Loss: 1.6755332789485393, Val Loss: 1.6641346532408205\n",
      "Epoch: 45, Loss: 1.664894031317489, Val Loss: 1.6536851578797016\n",
      "Epoch: 46, Loss: 1.6544242368783137, Val Loss: 1.643400540111351\n",
      "Epoch: 47, Loss: 1.6441162536813554, Val Loss: 1.6332767557497347\n",
      "Epoch: 48, Loss: 1.6339648246148102, Val Loss: 1.6233105912561723\n",
      "Epoch: 49, Loss: 1.623968823871958, Val Loss: 1.613493795192048\n",
      "Epoch: 50, Loss: 1.6141260509757367, Val Loss: 1.6038253233709148\n",
      "Epoch: 51, Loss: 1.6044319944566496, Val Loss: 1.5943019672387613\n",
      "Epoch: 52, Loss: 1.594883118446879, Val Loss: 1.5849196939741002\n",
      "Epoch: 53, Loss: 1.5854759280738564, Val Loss: 1.575675584763297\n",
      "Epoch: 54, Loss: 1.576206774061913, Val Loss: 1.566569539717067\n",
      "Epoch: 55, Loss: 1.567074934640525, Val Loss: 1.5575994525606216\n",
      "Epoch: 56, Loss: 1.558077282463876, Val Loss: 1.5487584723155845\n",
      "Epoch: 57, Loss: 1.5492086675786005, Val Loss: 1.5400451839512699\n",
      "Epoch: 58, Loss: 1.5404683547280251, Val Loss: 1.5314589185477834\n",
      "Epoch: 59, Loss: 1.5318534329993123, Val Loss: 1.5229966722512358\n",
      "Epoch: 60, Loss: 1.5233611289879125, Val Loss: 1.5146567055900857\n",
      "Epoch: 61, Loss: 1.514991227350607, Val Loss: 1.5064353464212588\n",
      "Epoch: 62, Loss: 1.5067393927669628, Val Loss: 1.4983300857101434\n",
      "Epoch: 63, Loss: 1.4986053234827243, Val Loss: 1.490338930797503\n",
      "Epoch: 64, Loss: 1.4905865992255385, Val Loss: 1.4824611254985467\n",
      "Epoch: 65, Loss: 1.4826818904662329, Val Loss: 1.474695073500395\n",
      "Epoch: 66, Loss: 1.4748878289137837, Val Loss: 1.467037343090556\n",
      "Epoch: 67, Loss: 1.4672014620652623, Val Loss: 1.4594862665734811\n",
      "Epoch: 68, Loss: 1.4596217385446304, Val Loss: 1.4520414470876224\n",
      "Epoch: 69, Loss: 1.4521501973508713, Val Loss: 1.4447018339218891\n",
      "Epoch: 70, Loss: 1.4447836308583974, Val Loss: 1.4374640781475372\n",
      "Epoch: 71, Loss: 1.4375198350878875, Val Loss: 1.4303286446225896\n",
      "Epoch: 72, Loss: 1.4303582030162418, Val Loss: 1.4232928276530366\n",
      "Epoch: 73, Loss: 1.4232950577277526, Val Loss: 1.4163550910848883\n",
      "Epoch: 74, Loss: 1.4163286249146585, Val Loss: 1.4095146437023047\n",
      "Epoch: 75, Loss: 1.4094579017016156, Val Loss: 1.4027682870054365\n",
      "Epoch: 76, Loss: 1.4026812497514631, Val Loss: 1.39611333778703\n",
      "Epoch: 77, Loss: 1.3959963473129091, Val Loss: 1.389549755485896\n",
      "Epoch: 78, Loss: 1.389403790478093, Val Loss: 1.3830774159044459\n",
      "Epoch: 79, Loss: 1.3829020532246223, Val Loss: 1.376692748129542\n",
      "Epoch: 80, Loss: 1.376488383563682, Val Loss: 1.3703955779563182\n",
      "Epoch: 81, Loss: 1.370162611528023, Val Loss: 1.3641843623636987\n",
      "Epoch: 82, Loss: 1.3639230859677793, Val Loss: 1.3580567384316895\n",
      "Epoch: 83, Loss: 1.3577679830331815, Val Loss: 1.3520127348787132\n",
      "Epoch: 84, Loss: 1.3516973124643648, Val Loss: 1.346050449010567\n",
      "Epoch: 85, Loss: 1.3457096880031545, Val Loss: 1.340168462141647\n",
      "Epoch: 86, Loss: 1.3398031622559925, Val Loss: 1.3343627688930453\n",
      "Epoch: 87, Loss: 1.3339746825260212, Val Loss: 1.328633082995175\n",
      "Epoch: 88, Loss: 1.3282244620963324, Val Loss: 1.3229779476285843\n",
      "Epoch: 89, Loss: 1.3225504428731107, Val Loss: 1.3173976678438624\n",
      "Epoch: 90, Loss: 1.3169522394912196, Val Loss: 1.311890272799209\n",
      "Epoch: 91, Loss: 1.3114273963353833, Val Loss: 1.306454761575355\n",
      "Epoch: 92, Loss: 1.3059747388124745, Val Loss: 1.3010886025841903\n",
      "Epoch: 93, Loss: 1.3005936770613034, Val Loss: 1.2957938597458512\n",
      "Epoch: 94, Loss: 1.2952844637833802, Val Loss: 1.29056866403857\n",
      "Epoch: 95, Loss: 1.290044839566774, Val Loss: 1.2854118116988464\n",
      "Epoch: 96, Loss: 1.284874315920118, Val Loss: 1.2803221219460281\n",
      "Epoch: 97, Loss: 1.2797714121276285, Val Loss: 1.2752980248931207\n",
      "Epoch: 98, Loss: 1.2747337038203417, Val Loss: 1.2703393617694676\n",
      "Epoch: 99, Loss: 1.2697603134734805, Val Loss: 1.2654445957515705\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "# input_neurons: Number of input features (28×28 = 784 pixels)\n",
    "# hidden_neurons: Number of neurons in the hidden layer (controls model capacity)\n",
    "# output_neurons: Number of classes to predict (10 fashion categories)\n",
    "# learning_rate: Step size for weight updates during training (0.01 balances speed and stability)\n",
    "# epochs: Number of full passes through the training data (100 cycles)\n",
    "\n",
    "# Optimized hyperparameters for better performance\n",
    "fashion_mnist = NN(784, 128, 10, 0.01, 100)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val     = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test   = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "# Training the model\n",
    "train_loss, val_loss = fashion_mnist.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(val_loss,   label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "# We use np.argmax to convert the model’s probability outputs (arrays of length 10)\n",
    "# into discrete class labels by selecting the index with the highest probability.\n",
    "# The model output is a probability distribution per sample; argmax picks the predicted class.\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {np.mean(y_pred == y_test)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
